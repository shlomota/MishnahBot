{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d47679d-83be-4fa3-911a-a388c4b31897",
   "metadata": {},
   "source": [
    "# Install Dependencies and Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f6da38-777d-40d3-b1f9-3362ef8f2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb tqdm langchain chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d4fa9f-bde9-4194-8639-c3c7028fc144",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git init sefaria-json\n",
    "%cd sefaria-json\n",
    "!git sparse-checkout init --cone\n",
    "!git sparse-checkout set json\n",
    "!git remote add origin https://github.com/Sefaria/Sefaria-Export.git\n",
    "!git pull origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e949aa-bc60-4408-8340-051df12b8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p new_directory\n",
    "!find Mishnah/Seder* -name \"merged.json\" -exec cp --parents \\{\\} new_directory/ \\;\n",
    "!apt install tree\n",
    "!tree Mishna/ | less"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc72bb2e-0e36-472c-8543-b9ce7024f551",
   "metadata": {},
   "source": [
    "# Loading the Dataset\n",
    "\n",
    "This code block loads the Mishnah dataset from the Sefaria-Export repository and creates a Pandas DataFrame with the relevant information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb5ced-473d-4db6-b1f9-28f74828bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to load all documents into a DataFrame with progress bar\n",
    "def load_documents(base_path):\n",
    "    data = []\n",
    "    for seder in tqdm(os.listdir(base_path), desc=\"Loading Seders\"):\n",
    "        seder_path = os.path.join(base_path, seder)\n",
    "        if os.path.isdir(seder_path):\n",
    "            for tractate in tqdm(os.listdir(seder_path), desc=f\"Loading Tractates in {seder}\", leave=False):\n",
    "                tractate_path = os.path.join(seder_path, tractate)\n",
    "                if os.path.isdir(tractate_path):\n",
    "                    english_file = os.path.join(tractate_path, \"English\", \"merged.json\")\n",
    "                    hebrew_file = os.path.join(tractate_path, \"Hebrew\", \"merged.json\")\n",
    "                    if os.path.exists(english_file) and os.path.exists(hebrew_file):\n",
    "                        with open(english_file, 'r', encoding='utf-8') as ef, open(hebrew_file, 'r', encoding='utf-8') as hf:\n",
    "                            english_data = json.load(ef)\n",
    "                            hebrew_data = json.load(hf)\n",
    "                            for chapter_index, (english_chapter, hebrew_chapter) in enumerate(zip(english_data['text'], hebrew_data['text'])):\n",
    "                                for mishnah_index, (english_paragraph, hebrew_paragraph) in enumerate(zip(english_chapter, hebrew_chapter)):\n",
    "                                    data.append({\n",
    "                                        \"seder\": seder,\n",
    "                                        \"tractate\": tractate,\n",
    "                                        \"chapter\": chapter_index + 1,\n",
    "                                        \"mishnah\": mishnah_index + 1,\n",
    "                                        \"english\": english_paragraph,\n",
    "                                        \"hebrew\": hebrew_paragraph\n",
    "                                    })\n",
    "    return pd.DataFrame(data)\n",
    "# Load all documents\n",
    "base_path = \"Mishnah\"\n",
    "df = load_documents(base_path)\n",
    "# Save the DataFrame to a file for future reference\n",
    "df.to_csv(os.path.join(base_path, \"mishnah_metadata.csv\"), index=False)\n",
    "print(\"Dataset successfully loaded into DataFrame and saved to file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b0a8d-3726-4c28-8a85-24a2cf210550",
   "metadata": {},
   "source": [
    "# Vectorizing and Storing in ChromaDB\n",
    "\n",
    "This code block vectorizes the text data using the 'all-MiniLM-L6-v2' sentence transformer model and stores the embeddings and metadata in a ChromaDB collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e0d841-07fe-4d45-b151-dd6571091d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "# Initialize ChromaDB\n",
    "chroma_client = chromadb.Client(Settings(persist_directory=\"chroma_db\"))\n",
    "collection = chroma_client.create_collection(\"mishnah\")\n",
    "# Load the dataset from the saved file\n",
    "df = pd.read_csv(os.path.join(\"Mishnah\", \"mishnah_metadata.csv\"))\n",
    "# Function to generate embeddings with progress bar\n",
    "def generate_embeddings(paragraphs, model):\n",
    "    embeddings = []\n",
    "    for paragraph in tqdm(paragraphs, desc=\"Generating Embeddings\"):\n",
    "        embedding = model.encode(paragraph, show_progress_bar=False)\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings)\n",
    "# Generate embeddings for English paragraphs\n",
    "embeddings = generate_embeddings(df['english'].tolist(), model)\n",
    "df['embedding'] = embeddings.tolist()\n",
    "# Store embeddings in ChromaDB with progress bar\n",
    "for index, row in tqdm(df.iterrows(), desc=\"Storing in ChromaDB\", total=len(df)):\n",
    "    collection.add(embeddings=[row['embedding']], documents=[row['english']], metadatas=[{\n",
    "        \"seder\": row['seder'],\n",
    "        \"tractate\": row['tractate'],\n",
    "        \"chapter\": row['chapter'],\n",
    "        \"mishnah\": row['mishnah'],\n",
    "        \"hebrew\": row['hebrew']\n",
    "    }])\n",
    "print(\"Embeddings and metadata successfully stored in ChromaDB.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8311a210-b9be-4176-aada-4b1f989edaff",
   "metadata": {},
   "source": [
    "# Creating Our RAG in English\n",
    "\n",
    "This code block sets up the Retrieval-Augmented Generation (RAG) system in English using LangChain, AWS Bedrock, and the 'all-MiniLM-L6-v2' embedding model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8226cb18-b344-4356-af42-795ac27b0742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain.llms import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from typing import List\n",
    "\n",
    "# Initialize AWS Bedrock for Llama 3 70B Instruct\n",
    "llm = Bedrock(\n",
    "    model_id=\"meta.llama3-70b-instruct-v1:0\"\n",
    ")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    Answer the following question based on the provided context alone:\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer (short and concise):\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# Initialize ChromaDB\n",
    "chroma_client = chromadb.Client(Settings(persist_directory=\"chroma_db\"))\n",
    "collection = chroma_client.get_collection(\"mishnah\")\n",
    "\n",
    "# Define the embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "\n",
    "# Define a simple retriever function\n",
    "def simple_retriever(query: str, k: int = 3) -> List[str]:\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=k)\n",
    "    documents = results['documents'][0]  # Access the first list inside 'documents'\n",
    "    sources = results['metadatas'][0]  # Access the metadata for sources\n",
    "    return documents, sources\n",
    "\n",
    "# Initialize the LLM chain\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template\n",
    ")\n",
    "\n",
    "# Define SimpleQA chain\n",
    "class SimpleQAChain:\n",
    "    def __init__(self, retriever, llm_chain):\n",
    "        self.retriever = retriever\n",
    "        self.llm_chain = llm_chain\n",
    "\n",
    "    def __call__(self, inputs, do_print_context=True):\n",
    "        question = inputs[\"query\"]\n",
    "        retrieved_docs, sources = self.retriever(question)\n",
    "        context = \"\\n\\n\".join(retrieved_docs)\n",
    "        response = self.llm_chain.run({\"context\": context, \"question\": question})\n",
    "        response_with_sources = f\"{response}\\n\" + \"#\"*50 + \"\\nSources:\\n\" + \"\\n\".join(\n",
    "            [f\"{source['seder']} {source['tractate']} Chapter {source['chapter']}, Mishnah {source['mishnah']}\" for source in sources]\n",
    "        )\n",
    "        if do_print_context:\n",
    "            print(\"#\"*50)\n",
    "            print(\"Retrieved paragraphs:\")\n",
    "            for doc in retrieved_docs:\n",
    "                print(doc[:100] + \"...\")\n",
    "        return response_with_sources\n",
    "\n",
    "# Initialize and test SimpleQAChain\n",
    "qa_chain = SimpleQAChain(retriever=simple_retriever, llm_chain=llm_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9223812-88eb-48f1-a016-64b668e216cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_chain({\"query\": \"What is the appropriate time to recite Shema?\"})\n",
    "\n",
    "print(\"#\"*50)\n",
    "print(\"Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2382ea95-eca6-4f27-b372-a88b8efcd076",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_chain({\"query\": \"What is the third prohibited kind of work on the sabbbath?\"})\n",
    "\n",
    "print(\"#\"*50)\n",
    "print(\"Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ab7421-0229-4dbf-aba4-b59bc130990c",
   "metadata": {},
   "source": [
    "# Multilingual RAG Approach\n",
    "\n",
    "This code block sets up a multilingual RAG system that supports Hebrew queries by translating them to English, retrieving relevant documents using English embeddings, and generating responses in Hebrew using the retrieved Hebrew context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eab6af-9661-4945-9c33-4398c016f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain.llms import Bedrock\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "# Initialize AWS Bedrock for Llama 3 70B Instruct with specific configurations for translation\n",
    "translation_llm = Bedrock(\n",
    "    model_id=\"meta.llama3-70b-instruct-v1:0\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.0,  # Set lower temperature for translation\n",
    "        \"max_gen_len\": 50  # Limit number of tokens for translation\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize AWS Bedrock for Claude Sonnet with specific configurations for generation\n",
    "generation_llm = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    ")\n",
    "\n",
    "# Define the translation prompt template\n",
    "translation_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"\"\"Translate the following Hebrew text to English:\n",
    "    Input text: {text}\n",
    "    Translation: \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Define the prompt template for Hebrew answers\n",
    "hebrew_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"ענה על השאלה הבאה בהתבסס על ההקשר המסופק בלבד:\n",
    "    הקשר: {context}\n",
    "    שאלה: {question}\n",
    "    תשובה (קצרה ותמציתית):\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Initialize ChromaDB\n",
    "chroma_client = chromadb.Client(Settings(persist_directory=\"chroma_db\"))\n",
    "collection = chroma_client.get_collection(\"mishnah\")\n",
    "\n",
    "# Define the embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "\n",
    "# Translation chain for translating queries from Hebrew to English\n",
    "translation_chain = LLMChain(\n",
    "    llm=translation_llm,\n",
    "    prompt=translation_prompt_template\n",
    ")\n",
    "\n",
    "# Initialize the LLM chain for Hebrew answers\n",
    "hebrew_llm_chain = LLMChain(\n",
    "    llm=generation_llm,\n",
    "    prompt=hebrew_prompt_template\n",
    ")\n",
    "\n",
    "# Define a simple retriever function for Hebrew texts\n",
    "def simple_retriever(query: str, k: int = 3) -> List[str]:\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=k)\n",
    "    documents = [meta['hebrew'] for meta in results['metadatas'][0]]  # Access Hebrew texts\n",
    "    sources = results['metadatas'][0]  # Access the metadata for sources\n",
    "    return documents, sources\n",
    "\n",
    "# Function to remove vowels from Hebrew text\n",
    "def remove_vowels_hebrew(hebrew_text):\n",
    "    pattern = re.compile(r'[\\u0591-\\u05C7]')\n",
    "    hebrew_text_without_vowels = re.sub(pattern, '', hebrew_text)\n",
    "    return hebrew_text_without_vowels\n",
    "\n",
    "# Define SimpleQA chain with translation\n",
    "class SimpleQAChainWithTranslation:\n",
    "    def __init__(self, translation_chain, retriever, llm_chain):\n",
    "        self.translation_chain = translation_chain\n",
    "        self.retriever = retriever\n",
    "        self.llm_chain = llm_chain\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        hebrew_query = inputs[\"query\"]\n",
    "        print(\"#\" * 50)\n",
    "        print(f\"Hebrew query: {hebrew_query}\")\n",
    "        \n",
    "        # Print the translation prompt\n",
    "        translation_prompt = translation_prompt_template.format(text=hebrew_query)\n",
    "        print(\"#\" * 50)\n",
    "        print(f\"Translation Prompt: {translation_prompt}\")\n",
    "        \n",
    "        # Perform the translation using the translation chain with specific configurations\n",
    "        translated_query = self.translation_chain.run({\"text\": hebrew_query})\n",
    "        print(\"#\" * 50)\n",
    "        print(f\"Translated Query: {translated_query}\")  # Print the translated query for debugging\n",
    "        \n",
    "        retrieved_docs, sources = self.retriever(translated_query)\n",
    "        retrieved_docs = [remove_vowels_hebrew(doc) for doc in retrieved_docs]\n",
    "\n",
    "        context = \"\\n\".join(retrieved_docs)\n",
    "        \n",
    "        # Print the final prompt for generation\n",
    "        final_prompt = hebrew_prompt_template.format(context=context, question=hebrew_query)\n",
    "        print(\"#\" * 50)\n",
    "        print(f\"Final Prompt for Generation:\\n {final_prompt}\")\n",
    "        \n",
    "        response = self.llm_chain.run({\"context\": context, \"question\": hebrew_query})\n",
    "        response_with_sources = f\"{response}\\n\" + \"#\" * 50 + \"מקורות:\\n\" + \"\\n\".join(\n",
    "            [f\"{source['seder']} {source['tractate']} פרק {source['chapter']}, משנה {source['mishnah']}\" for source in sources]\n",
    "        )\n",
    "        return response_with_sources\n",
    "\n",
    "# Initialize and test SimpleQAChainWithTranslation\n",
    "qa_chain = SimpleQAChainWithTranslation(translation_chain, simple_retriever, hebrew_llm_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41175a9-9b79-40c7-9faa-8c018e1d419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_chain({\"query\": \"מהו סוג העבודה השלישי האסור בשבת?\"})\n",
    "print(\"#\" * 50)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
